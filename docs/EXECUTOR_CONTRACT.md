**1ï¸âƒ£ Executor invocation overview**

An Executor is invoked only after:
user message is persisted
Agent returns action = execute
Backend approves execution

Executors:
never talk to the database
never talk to the frontend
never decide if they should run

**2ï¸âƒ£ Executor INPUT (Backend â†’ Executor)**
ğŸ”¹ High-level structure
{
  "task_id": "uuid",
  "executor_type": "llm",
  "task": { ... },
  "execution_context": { ... },
  "constraints": { ... }
}

ğŸ”¹ task_id
"task_id": "uuid-v4"


Purpose:
tracing
retries
logging
correlating outputs
Generated by backend.

ğŸ”¹ executor_type
"executor_type": "llm"


Allowed values (extensible):

llm
template_engine
document_processor
classifier
tool_call
Backend routes based on this.

ğŸ”¹ task

This is what to do, not how.
Example for LLM:

"task": {
  "task_type": "generate_template",
  "prompt": "Generate a supplier contract template...",
  "input_variables": {
    "supplier_name": "ACME Ltd",
    "jurisdiction": "Denmark"
  }
}


Notes:

Agent suggests structure
Backend finalizes prompt
Executor does not infer intent

ğŸ”¹ execution_context
"execution_context": {
  "project_id": 1,
  "conversation_snippet": [
    { "role": "user", "content": "..." },
    { "role": "assistant", "content": "..." }
  ],
  "retrieved_documents": [
    { "title": "Supplier Contract v2", "snippet": "..." }
  ]
}


Purpose:

give executor context
executor does NOT fetch memory itself
backend controls data exposure

ğŸ”¹ constraints
"constraints": {
  "max_tokens": 2000,
  "temperature": 0.3,
  "allowed_models": ["gpt-4.1", "local-llm-v1"],
  "timeout_ms": 30000
}


Purpose:

cost control
safety
predictability
Backend may override Agent suggestions.

**3ï¸âƒ£ Executor OUTPUT (Executor â†’ Backend)**
ğŸ”¹ High-level structure
{
  "status": "success",
  "result": { ... },
  "usage": { ... },
  "model_used": "gpt-4.1"
}

ğŸ”¹ status

Allowed values:

success
failed
partial
Executor never throws raw errors to backend.

ğŸ”¹ result

Example (LLM):

"result": {
  "content": "Here is the supplier contract template...",
  "format": "markdown"
}


Executor outputs content only, no DB logic.

ğŸ”¹ usage
"usage": {
  "tokens_input": 820,
  "tokens_output": 1340,
  "total_tokens": 2160,
  "estimated_cost": 0.21
}


Purpose:
billing
analytics
auditing
Backend trusts but verifies.

ğŸ”¹ model_used
"model_used": "gpt-4.1"


Important for:

cost tracking
debugging
fallback logic

**4ï¸âƒ£ Failure handling (VERY IMPORTANT)**

If executor fails:

{
  "status": "failed",
  "error": {
    "code": "MODEL_TIMEOUT",
    "message": "Execution exceeded 30s"
  }
}


Backend decides:

retry
fallback model
user-facing message
logging
Executor does not retry on its own.

**5ï¸âƒ£ Why executors must be dumb**

Executors should be:
stateless
deterministic
replaceable
testable in isolation
All intelligence stays in:
Agent (decision)
Backend (rules & persistence)

**6ï¸âƒ£ Example end-to-end flow (mental model)**

User sends message
/api/chat persists it
Backend calls Agent
Agent returns execute
Backend selects executor
Executor runs
Backend saves assistant message
Frontend updates UI
Executors never see steps 1â€“4.